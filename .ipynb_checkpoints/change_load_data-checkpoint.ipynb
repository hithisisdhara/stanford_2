{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import codecs\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "\n",
    "# input: a sequence of tokens, and a token_to_index dictionary\n",
    "# output: a LongTensor variable to encode the sequence of idxs\n",
    "def prepare_sequence(seq, to_ix, cuda=False):\n",
    "    var = autograd.Variable(torch.LongTensor([to_ix[w] for w in seq.split(' ')]))\n",
    "    return var\n",
    "\n",
    "def prepare_label(label,label_to_ix, cuda=False):\n",
    "    var = autograd.Variable(torch.LongTensor([label_to_ix[label]]))\n",
    "    return var\n",
    "\n",
    "def build_token_to_ix(sentences):\n",
    "    token_to_ix = dict()\n",
    "    print(len(sentences))\n",
    "    for sent in sentences:\n",
    "        for token in sent.split(' '):\n",
    "            if token not in token_to_ix:\n",
    "                token_to_ix[token] = len(token_to_ix)\n",
    "    token_to_ix['<pad>'] = len(token_to_ix)\n",
    "    return token_to_ix\n",
    "\n",
    "def build_label_to_ix(labels):\n",
    "    label_to_ix = dict()\n",
    "    for label in labels:\n",
    "        if label not in label_to_ix:\n",
    "            label_to_ix[label] = len(label_to_ix)\n",
    "\n",
    "\n",
    "def get_all_files_from_dir(dirpath):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    onlyfiles = [f for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "    return onlyfiles\n",
    "\n",
    "def head_n(fname, n=2):\n",
    "    count = 1\n",
    "    print '---------------------------------------'\n",
    "    f = open(fname)\n",
    "    for line in f:\n",
    "        print line\n",
    "        count += 1\n",
    "        if count > n:\n",
    "            print '-----------------------------------'\n",
    "            f.close()\n",
    "            return\n",
    "#head_n(test_file_pos,10)\n",
    "#thos function would only extract vp and vn files, in order to extract vpn = vp+vn in one file, give last arguement as False/0 \n",
    "\n",
    "def extract_names(l_files,patt,p_xor_n = True):\n",
    "    # note that you may need to \n",
    "    r = []\n",
    "    for f in l_files:\n",
    "        tokens = f.split(\".\")\n",
    "        if tokens[-3]==patt:\n",
    "            if p_xor_n:\n",
    "                if tokens[-2] != 'vpn':\n",
    "                    r.append(f)\n",
    "                    #yield f\n",
    "            elif tokens[-2] == 'vpn':\n",
    "                return f\n",
    "    return sorted(r)\n",
    "#extract_names(files,'test')\n",
    "def get_sentence_out(path):\n",
    "    f = open(path)\n",
    "    return map(lambda x:x.split(\",\")[2],f)\n",
    "def get_neg_pos_sent(type_,files,path):\n",
    "    return [get_sentence_out(path+n) for n in extract_names(files,type_)]\n",
    "def load_stanford_data():\n",
    "    fpath = './cross_validation_data/vpn_filtered/'\n",
    "    files = get_all_files_from_dir(fpath)\n",
    "    \n",
    "    train_sent_neg,train_sent_pos = get_neg_pos_sent('train',files,fpath)\n",
    "    val_sent_neg,val_sent_pos = get_neg_pos_sent('dev',files,fpath)\n",
    "    test_sent_neg,test_sent_pos = get_neg_pos_sent('test',files,fpath)\n",
    "    \n",
    "    train_data = [(sent,1) for sent in train_sent_pos] + [(sent, 0) for sent in train_sent_neg]\n",
    "    dev_data = [(sent, 1) for sent in val_sent_pos] + [(sent, 0) for sent in val_sent_neg]\n",
    "    test_data = [(sent, 1) for sent in test_sent_pos] + [(sent, 0) for sent in test_sent_neg]\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(dev_data)\n",
    "    random.shuffle(test_data)\n",
    "\n",
    "    print('train:',len(train_data),'dev:',len(dev_data),'test:',len(test_data))\n",
    "    \n",
    "    word_to_ix = build_token_to_ix([s for s,_ in train_data+dev_data+test_data])\n",
    "    label_to_ix = {0:0,1:1}\n",
    "    print('vocab size:',len(word_to_ix),'label size:',len(label_to_ix))\n",
    "    print('loading data done!')\n",
    "    return train_data,dev_data,test_data,word_to_ix,label_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train:', 25667, 'dev:', 292, 'test:', 657)\n",
      "26616\n",
      "('vocab size:', 14378, 'label size:', 2)\n",
      "loading data done!\n"
     ]
    }
   ],
   "source": [
    "train_data,dev_data,test_data,word_to_ix,label_to_ix = load_stanford_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
