{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train:', 25667, 'dev:', 292, 'test:', 657)\n",
      "26616\n",
      "('vocab size:', 14378, 'label size:', 2)\n",
      "loading data done!\n",
      "epoch: 3 start!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dharashah/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:144: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "torch.set_num_threads(8)\n",
    "import sys\n",
    "import codecs\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "import copy\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "# input: a sequence of tokens, and a token_to_index dictionary\n",
    "# output: a LongTensor variable to encode the sequence of idxs\n",
    "def prepare_sequence(seq, to_ix, cuda=False):\n",
    "    var = autograd.Variable(torch.LongTensor([to_ix[w] for w in seq.split(' ')]))\n",
    "    return var\n",
    "\n",
    "def prepare_label(label,label_to_ix, cuda=False):\n",
    "    var = autograd.Variable(torch.LongTensor([label_to_ix[label]]))\n",
    "    return var\n",
    "\n",
    "def build_token_to_ix(sentences):\n",
    "    token_to_ix = dict()\n",
    "    print(len(sentences))\n",
    "    for sent in sentences:\n",
    "        for token in sent.split(' '):\n",
    "            if token not in token_to_ix:\n",
    "                token_to_ix[token] = len(token_to_ix)\n",
    "    token_to_ix['<pad>'] = len(token_to_ix)\n",
    "    return token_to_ix\n",
    "\n",
    "def build_label_to_ix(labels):\n",
    "    label_to_ix = dict()\n",
    "    for label in labels:\n",
    "        if label not in label_to_ix:\n",
    "            label_to_ix[label] = len(label_to_ix)\n",
    "\n",
    "\n",
    "def get_all_files_from_dir(dirpath):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    onlyfiles = [f for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "    return onlyfiles\n",
    "\n",
    "def head_n(fname, n=2):\n",
    "    count = 1\n",
    "    print ('---------------------------------------')\n",
    "    f = open(fname)\n",
    "    for line in f:\n",
    "        print (line)\n",
    "        count += 1\n",
    "        if count > n:\n",
    "            print ('-----------------------------------')\n",
    "            f.close()\n",
    "            return\n",
    "#head_n(test_file_pos,10)\n",
    "#thos function would only extract vp and vn files, in order to extract vpn = vp+vn in one file, give last arguement as False/0 \n",
    "\n",
    "def extract_names(l_files,patt,p_xor_n = True):\n",
    "    # note that you may need to \n",
    "    r = []\n",
    "    for f in l_files:\n",
    "        tokens = f.split(\".\")\n",
    "        if tokens[-3]==patt:\n",
    "            if p_xor_n:\n",
    "                if tokens[-2] != 'vpn':\n",
    "                    r.append(f)\n",
    "                    #yield f\n",
    "            elif tokens[-2] == 'vpn':\n",
    "                return f\n",
    "    return sorted(r)\n",
    "#extract_names(files,'test')\n",
    "def get_sentence_out(path):\n",
    "    f = open(path)\n",
    "    return map(lambda x:x.split(\",\")[2],f)\n",
    "def get_neg_pos_sent(type_,files,path):\n",
    "    return [get_sentence_out(path+n) for n in extract_names(files,type_)]\n",
    "def load_stanford_data():\n",
    "    fpath = './cross_validation_data/vpn_filtered/'\n",
    "    files = get_all_files_from_dir(fpath)\n",
    "    \n",
    "    train_sent_neg,train_sent_pos = get_neg_pos_sent('train',files,fpath)\n",
    "    val_sent_neg,val_sent_pos = get_neg_pos_sent('dev',files,fpath)\n",
    "    test_sent_neg,test_sent_pos = get_neg_pos_sent('test',files,fpath)\n",
    "    \n",
    "    train_data = [(sent,1) for sent in train_sent_pos] + [(sent, 0) for sent in train_sent_neg]\n",
    "    dev_data = [(sent, 1) for sent in val_sent_pos] + [(sent, 0) for sent in val_sent_neg]\n",
    "    test_data = [(sent, 1) for sent in test_sent_pos] + [(sent, 0) for sent in test_sent_neg]\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(dev_data)\n",
    "    random.shuffle(test_data)\n",
    "\n",
    "    print('train:',len(train_data),'dev:',len(dev_data),'test:',len(test_data))\n",
    "    \n",
    "    word_to_ix = build_token_to_ix([s for s,_ in train_data+dev_data+test_data])\n",
    "    label_to_ix = {0:0,1:1}\n",
    "    print('vocab size:',len(word_to_ix),'label size:',len(label_to_ix))\n",
    "    print('loading data done!')\n",
    "    return train_data,dev_data,test_data,word_to_ix,label_to_ix\n",
    "\n",
    "# Load the data and define hyperparametars and specifications ###########################\n",
    "train_data, dev_data, test_data, word_to_ix, label_to_ix = load_stanford_data()\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "EPOCH = 20\n",
    "NUM_LAYERS = 2\n",
    "loss_function = nn.NLLLoss()#negative log likelihood loss \n",
    "########################################################################################\n",
    "\n",
    "\n",
    "#Define the BiLSTM class################################################################\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, label_size):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,num_layers,bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(2*hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers*2, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(self.num_layers*2, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        x = embeds.view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(y)\n",
    "        return log_probs\n",
    "########################################################################################\n",
    "\n",
    "#Define the helper functions for the accuracy###########################################\n",
    "def get_accuracy(truth, pred):\n",
    "     assert len(truth)==len(pred)\n",
    "     right = 0\n",
    "     for i in range(len(truth)):\n",
    "         if truth[i]==pred[i]:\n",
    "             right += 1.0\n",
    "     return right/len(truth)\n",
    "\n",
    "def evaluate(model, data, loss_function, word_to_ix, label_to_ix, name ='dev'):\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "\n",
    "    for sent, label in data:\n",
    "        truth_res.append(label_to_ix[label])\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = prepare_sequence(sent, word_to_ix)\n",
    "        label = prepare_label(label, label_to_ix)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        # model.zero_grad() # should I keep this when I am evaluating the model?\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "    avg_loss /= len(data)\n",
    "    acc = get_accuracy(truth_res, pred_res)\n",
    "    print(name + ' avg_loss:%g train acc:%g' % (avg_loss, acc ))\n",
    "    return acc\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "#Define training per epoch###########################################################\n",
    "\n",
    "def train_epoch(model, train_data, loss_function, optimizer, word_to_ix, label_to_ix, i):\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = 0.0\n",
    "    count = 0\n",
    "    truth_res = []\n",
    "    pred_res = []\n",
    "    batch_sent = []\n",
    "\n",
    "    for sent, label in train_data:\n",
    "\n",
    "\n",
    "        truth_res.append(label_to_ix[label])\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        sent = prepare_sequence(sent, word_to_ix)\n",
    "        label = prepare_label(label, label_to_ix)\n",
    "        pred = model(sent)\n",
    "        pred_label = pred.data.max(1)[1].numpy()\n",
    "        pred_res.append(pred_label)\n",
    "        model.zero_grad()\n",
    "        loss = loss_function(pred, label)\n",
    "        avg_loss += loss.data[0]\n",
    "        count += 1\n",
    "        if count % 500 == 0:\n",
    "            print('epoch: %d iterations: %d loss :%g' % (i, count, loss.data[0]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss /= len(train_data)\n",
    "    print('epoch: %d done! \\n train avg_loss:%g , acc:%g'%(i, avg_loss, get_accuracy(truth_res,pred_res)))\n",
    "########################################################################################\n",
    "# The main function that you call for training #########################################\n",
    "\n",
    "loaded_model = torch.load('./best_models/2deep_bilstm_best_model_epoch_2.model')\n",
    "model = BiLSTMClassifier(embedding_dim=EMBEDDING_DIM,hidden_dim=HIDDEN_DIM, num_layers = NUM_LAYERS,\n",
    "                               vocab_size=len(word_to_ix),label_size=len(label_to_ix))\n",
    "model.load_state_dict(copy.deepcopy(loaded_model))\n",
    "def train(model,start):\n",
    "    \n",
    "    best_dev_acc = 0.0\n",
    "    loss_function = nn.NLLLoss()#negative log likelihood loss \n",
    "    optimizer = optim.Adam(model.parameters(),lr = 1e-3)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
    "    \n",
    "    no_up = 0\n",
    "    for i in range(start+1,EPOCH+start):\n",
    "        random.shuffle(train_data)\n",
    "        print('epoch: %d start!' % i)\n",
    "        train_epoch(model, train_data, loss_function, optimizer, word_to_ix, label_to_ix, i)\n",
    "        print('now best dev acc:',best_dev_acc)\n",
    "        dev_acc = evaluate(model,dev_data,loss_function,word_to_ix,label_to_ix,'dev')\n",
    "        #dev_accs.append(dev_acc)\n",
    "        test_acc = evaluate(model, test_data, loss_function, word_to_ix, label_to_ix, 'test')\n",
    "        #test_accs.append(test_acc)\n",
    "        f = open('train_test.txt','a')\n",
    "        f.write(str(dev_acc)+\",\"+str(test_acc))\n",
    "        f.close()\n",
    "        torch.save(model.state_dict(), 'best_models/2deep_bilstm_best_model_epoch_'+str(i) + '.model')\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            print('New Best Dev!!!')\n",
    "            \n",
    "            no_up = 0\n",
    "        else:\n",
    "            no_up += 1\n",
    "            if no_up >= 10:\n",
    "                return model, i \n",
    "                exit()\n",
    "    return model,i\n",
    "#train~######\n",
    "model, i = train(model,2)\n",
    "#torch.save(model.state_dict(), 'best_models/bilstm_best_model_Epoch_'+str(i)+'_acc_' + str(int(test_acc*10000)) + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
